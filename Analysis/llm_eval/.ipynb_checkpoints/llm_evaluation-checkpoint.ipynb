{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc72981-06d2-4952-996f-787ef29f09ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47fa696-8688-45c7-9997-5002e5f1914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_csv = 'bb_test_sample.csv' #Path to the actual instance-feature file for a specific algorithm that is used for prediction by llm\n",
    "pred_csv   = './results_max_kp/gemini/Gemini_bb2.csv' #Path of the prediction output file\n",
    "output_csv = './llm50_results/gemini/bb_eval.csv' #Path where the output of this notebook will be saved\n",
    "\n",
    "\n",
    "actual_df = pd.read_csv(actual_csv)\n",
    "pred_df   = pd.read_csv(pred_csv)\n",
    "\n",
    "\n",
    "y_true_time    = actual_df['solution_time'].to_numpy()\n",
    "y_pred_time    = pred_df['pred_solution_time'].to_numpy()\n",
    "\n",
    "y_true_mem     = actual_df['peak_memory'].to_numpy()\n",
    "y_pred_mem     = pred_df['pred_memory_kb'].to_numpy()\n",
    "\n",
    "y_true_gap     = actual_df['optimality_gap'].to_numpy()\n",
    "y_pred_gap     = pred_df['pred_optimality_gap'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febebbe1-af97-42c6-8130-c21586238af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for name, y_true, y_pred in [\n",
    "    (\"solution_time\",  y_true_time, y_pred_time),\n",
    "    (\"peak_memory\",    y_true_mem,  y_pred_mem),\n",
    "    (\"optimality_gap\", y_true_gap,  y_pred_gap),\n",
    "]:\n",
    "    # Boolean mask: neither true nor pred is NaN\n",
    "    mask = (~np.isnan(y_true)) & (~np.isnan(y_pred))\n",
    "    y_t, y_p = y_true[mask], y_pred[mask]\n",
    "\n",
    "    mae  = mean_absolute_error(y_t, y_p)\n",
    "    mse = mean_squared_error(y_t, y_p)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2   = r2_score(y_t, y_p)\n",
    "\n",
    "    results.append({\n",
    "        'metric': name,\n",
    "        'n_samples': int(mask.sum()),\n",
    "        'MAE':  mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R2':   r2\n",
    "    })\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df.to_csv(output_csv, index=False)\n",
    "print(f\"Saved {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de3527-b143-40e5-8624-c99b57c3b58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
