{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9c2c2f5-eb8b-45dd-a2d3-4f51438592b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5af8fba-ae41-4389-884f-0e5280408531",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DPI   = \"./training_data/algorithms\"     #path to the folder that contains training dataset for algorithms\n",
    "BINS_BASE  = \"./trainingData/bins\"      #path to the folder that contains bins for algorithms\n",
    "CLASS_BASE = \"./outupt_folder\"                  \n",
    "K_LIST     = [3,5,7]                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eefc6b1-4652-4f16-bab0-346d0220ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classifier_proba(name, solver, target, X, X_cnn):\n",
    "\n",
    "    try:\n",
    "        if name == \"rf\":\n",
    "            p = f\"{CLASS_BASE}/rf_class/rf_classifier_models/rf_{solver}_{target}.joblib\"\n",
    "            if os.path.exists(p):\n",
    "                m = joblib.load(p); return name, m.predict_proba(X), p\n",
    "\n",
    "        if name == \"cb\":\n",
    "            p = f\"{CLASS_BASE}/cb_class/cb_classifier_models/cb_{solver}_{target}.cbm\"\n",
    "            if os.path.exists(p):\n",
    "                m = CatBoostClassifier(); m.load_model(p)\n",
    "                return name, m.predict_proba(X), p\n",
    "\n",
    "        if name == \"cnn\":\n",
    "           \n",
    "            pattern = f\"{CLASS_BASE}/cnn_class/cnn_classifier_models/cnn_classifier_{solver}_{target}_*e.h5\"\n",
    "            files   = glob.glob(pattern)\n",
    "            if not files:\n",
    "                return name, None, None\n",
    "            \n",
    "            fpath = sorted(files, key=lambda s: int(s.split(\"_\")[-1].rstrip(\"e.h5\")))[-1]\n",
    "            m = load_model(fpath, custom_objects={'mse': MeanSquaredError()})\n",
    "            return name, m.predict(X_cnn), fpath\n",
    "\n",
    "        if name == \"mlp\":\n",
    "            pattern = f\"{CLASS_BASE}/mlp_class/mlp_classifier_models/mlp_classifier_{solver}_{target}_*e.h5\"\n",
    "            files   = glob.glob(pattern)\n",
    "            if not files:\n",
    "                return name, None, None\n",
    "            fpath = sorted(files, key=lambda s: int(s.split(\"_\")[-1].rstrip(\"e.h5\")))[-1]\n",
    "            m = load_model(fpath, custom_objects={'mse': MeanSquaredError()})\n",
    "            return name, m.predict(X), fpath\n",
    "\n",
    "        if name == \"svm\":\n",
    "            p = f\"{CLASS_BASE}/svm_class/svm_classifier_models/svm_{solver}_{target}.joblib\"\n",
    "            if os.path.exists(p):\n",
    "                m = joblib.load(p); return name, m.predict_proba(X), p\n",
    "\n",
    "        if name == \"lr\":\n",
    "            p = f\"{CLASS_BASE}/lr_class/lr_classifier_models/lr_{solver}_{target}.joblib\"\n",
    "            if os.path.exists(p):\n",
    "                m = joblib.load(p); return name, m.predict_proba(X), p\n",
    "\n",
    "        if name == \"dt\":\n",
    "            p = f\"{CLASS_BASE}/dt_class/dt_classifier_models/dt_{solver}_{target}.joblib\"\n",
    "            if os.path.exists(p):\n",
    "                m = joblib.load(p); return name, m.predict_proba(X), p\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] loading {name}: {e}\")\n",
    "    return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae9517-6105-493c-a3ed-1368ad824870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_classifiers_for_solver(solver, train_file, test_file, val_file):\n",
    "    print(f\"\\n Solver: {solver} \")\n",
    "   \n",
    "    df_tr = pd.read_csv(train_file).dropna()\n",
    "    df_va = pd.read_csv(val_file).dropna()\n",
    "    df_te = pd.read_csv(test_file).dropna()\n",
    "\n",
    "   \n",
    "    bins_dir = os.path.join(BINS_BASE, f\"{solver}_bins\")\n",
    "    binf = glob.glob(os.path.join(bins_dir, \"*_bins.json\"))\n",
    "    assert len(binf)==1, \"Need one JSON in \"+bins_dir\n",
    "    bin_edges = json.load(open(binf[0]))\n",
    "\n",
    "    # scale features\n",
    "    feats = [\n",
    "      \"number_of_elements\",\"capacity\",\"max_weight\",\"min_weight\",\"mean_weight\",\n",
    "      \"median_weight\",\"std_weight\",\"weight_range\",\"max_profit\",\"min_profit\",\"mean_profit\",\n",
    "      \"median_profit\",\"std_profit\",\"profit_range\",\"renting_ratio\",\"mean_weight_profit_ratio\",\n",
    "      \"median_weight_profit_ratio\",\"capacity_mean_weight_ratio\",\"capacity_median_weight_ratio\",\n",
    "      \"capacity_std_weight_ratio\",\"std_weight_profit_ratio\",\"weight_profit_correlation\",\n",
    "      \"ram\",\"cpu_cores\"\n",
    "    ]\n",
    "    scaler = StandardScaler().fit(df_tr[feats])\n",
    "    X_va    = scaler.transform(df_va[feats])\n",
    "    X_te    = scaler.transform(df_te[feats])\n",
    "    X_va_cnn= X_va.reshape((-1,X_va.shape[1],1))\n",
    "    X_te_cnn= X_te.reshape((-1,X_te.shape[1],1))\n",
    "\n",
    "    records = []\n",
    "    for target in [\"solution_time\",\"optimality_gap\",\"peak_memory\"]:\n",
    "        # Recover true classes\n",
    "        edges = bin_edges[target]\n",
    "        def to_bins(arr,edges):\n",
    "            return np.clip(np.digitize(arr, edges[:-1], right=False)-1, 0, len(edges)-2)\n",
    "        y_va_raw = to_bins(df_va[target].values,edges)\n",
    "        y_te_raw = to_bins(df_te[target].values,edges)\n",
    "\n",
    "      \n",
    "        perf = {}\n",
    "        proba = {}\n",
    "        for name in [\"rf\",\"cb\",\"cnn\",\"mlp\",\"svm\",\"lr\",\"dt\"]:\n",
    "            nm, pr, path = load_classifier_proba(name, solver, target, X_va, X_va_cnn)\n",
    "            if pr is None: continue\n",
    "\n",
    "            y_va = np.clip(y_va_raw.copy(), 0, pr.shape[1] - 1)\n",
    "            y_te = np.clip(y_te_raw.copy(), 0, pr.shape[1] - 1)\n",
    "            ypred = pr.argmax(axis=1)\n",
    "            perf[nm] = f1_score(y_va, ypred, average=\"macro\", zero_division=0)\n",
    "            proba[nm] = pr\n",
    "            print(f\"  {nm:<3} val-F1 = {perf[nm]:.3f}\")\n",
    "\n",
    "        \n",
    "        if not perf:\n",
    "            print(f\"No classifiers for “{target}”, skip.\")\n",
    "            continue\n",
    "        # Rank by desc F1\n",
    "        ranked = sorted(perf, key=lambda m: perf[m], reverse=True)\n",
    " \n",
    "\n",
    "       \n",
    "        for K in K_LIST:\n",
    "            chosen = ranked[:K]\n",
    "\n",
    "           \n",
    "            all_probas = [\n",
    "                load_classifier_proba(m, solver, target, X_te, X_te_cnn)[1]\n",
    "                for m in chosen\n",
    "            ]\n",
    "            n_cls = max(pr.shape[1] for pr in all_probas)\n",
    "\n",
    "            padded = []\n",
    "            for pr in all_probas:\n",
    "                if pr.shape[1] < n_cls:\n",
    "                    pad_width = n_cls - pr.shape[1]\n",
    "                    pr = np.concatenate([pr, np.zeros((pr.shape[0], pad_width))], axis=1)\n",
    "                padded.append(pr)\n",
    "\n",
    "           \n",
    "            P = np.stack(padded, axis=0)      \n",
    "            P = P.mean(axis=0)                \n",
    "            y_pred = P.argmax(axis=1)\n",
    "\n",
    "            acc = accuracy_score(y_te, y_pred)\n",
    "            f1 = f1_score(y_te, y_pred, average=\"macro\", zero_division=0)\n",
    "            print(f\"Top-{K} ensemble test-F1 = {f1:.3f}\")\n",
    "\n",
    "            records.append({\n",
    "                \"solver\": solver,\n",
    "                \"target\": target,\n",
    "                \"Top_K\": K,\n",
    "                \"members\": \";\".join(chosen),\n",
    "                \"test_accuracy\": acc,\n",
    "                \"test_f1\": f1\n",
    "            })\n",
    "\n",
    "   \n",
    "    df_out = pd.DataFrame(records)\n",
    "    out_fp = os.path.join(CLASS_BASE, \"ensembles_fl\", f\"{solver}_class_ensembles.csv\")\n",
    "    os.makedirs(os.path.dirname(out_fp), exist_ok=True)\n",
    "    df_out.to_csv(out_fp, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc6f0cf1-8bf9-4f2b-bc04-87c7167a6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models(base_folder):\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        for folder in dirs:\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            csv_files = os.listdir(folder_path)\n",
    "\n",
    "            train_file = [f for f in csv_files if f.endswith(\"_train.csv\")]\n",
    "            test_file = [f for f in csv_files if f.endswith(\"_test.csv\")]\n",
    "            val_file = [f for f in csv_files if f.endswith(\"_val.csv\")]\n",
    "\n",
    "            if train_file and test_file and val_file:\n",
    "                train_fp = os.path.join(folder_path, train_file[0])\n",
    "                test_fp = os.path.join(folder_path, test_file[0])\n",
    "                val_fp = os.path.join(folder_path, val_file[0])\n",
    "\n",
    "                solver_name = folder  \n",
    "                ensemble_classifiers_for_solver(solver_name, train_fp, test_fp, val_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0023f338-7f3c-4612-8246-e798e4d9a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Solver: or_min ===\n",
      "  rf  val-F1 = 1.000\n",
      "  cb  val-F1 = 1.000\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
      "  cnn val-F1 = 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "  mlp val-F1 = 0.494\n",
      "  svm val-F1 = 1.000\n",
      "  lr  val-F1 = 1.000\n",
      "  dt  val-F1 = 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-3 ensemble test-F1 = 1.000\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "   ➞ Top-5 ensemble test-F1 = 1.000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "   ➞ Top-7 ensemble test-F1 = 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  rf  val-F1 = 0.487\n",
      "  cb  val-F1 = 0.487\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
      "  cnn val-F1 = 0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "  mlp val-F1 = 0.487\n",
      "  svm val-F1 = 0.487\n",
      "  lr  val-F1 = 0.487\n",
      "  dt  val-F1 = 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-3 ensemble test-F1 = 1.000\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "   ➞ Top-5 ensemble test-F1 = 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "   ➞ Top-7 ensemble test-F1 = 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  rf  val-F1 = 0.421\n",
      "  cb  val-F1 = 0.582\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "  cnn val-F1 = 0.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mlp val-F1 = 0.504\n",
      "  svm val-F1 = 0.547\n",
      "  lr  val-F1 = 0.318\n",
      "  dt  val-F1 = 0.553\n",
      "   ➞ Top-3 ensemble test-F1 = 0.553\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-5 ensemble test-F1 = 0.545\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "   ➞ Top-7 ensemble test-F1 = 0.497\n",
      "→ saved ./binres_min_kp/ensembles_fl/or_min_class_ensembles.csv\n",
      "\n",
      "=== Solver: gurobi_min ===\n",
      "  rf  val-F1 = 0.422\n",
      "  cb  val-F1 = 0.383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "  cnn val-F1 = 0.423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mlp val-F1 = 0.418\n",
      "  svm val-F1 = 0.449\n",
      "  lr  val-F1 = 0.483\n",
      "  dt  val-F1 = 0.425\n",
      "   ➞ Top-3 ensemble test-F1 = 0.480\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-5 ensemble test-F1 = 0.461\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-7 ensemble test-F1 = 0.421\n",
      "  ⚠️  No classifiers loaded for “optimality_gap”, skipping.\n",
      "  rf  val-F1 = 0.374\n",
      "  cb  val-F1 = 0.318\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "  cnn val-F1 = 0.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "  mlp val-F1 = 0.335\n",
      "  svm val-F1 = 0.370\n",
      "  lr  val-F1 = 0.208\n",
      "  dt  val-F1 = 0.270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step\n",
      "   ➞ Top-3 ensemble test-F1 = 0.220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "   ➞ Top-5 ensemble test-F1 = 0.216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "   ➞ Top-7 ensemble test-F1 = 0.298\n",
      "→ saved ./binres_min_kp/ensembles_fl/gurobi_min_class_ensembles.csv\n",
      "\n",
      "=== Solver: greedy_min ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  rf  val-F1 = 0.649\n",
      "  cb  val-F1 = 0.968\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "  cnn val-F1 = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
      "  mlp val-F1 = 0.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  svm val-F1 = 0.949\n",
      "  lr  val-F1 = 0.742\n",
      "  dt  val-F1 = 0.961\n",
      "   ➞ Top-3 ensemble test-F1 = 0.992\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
      "   ➞ Top-5 ensemble test-F1 = 0.992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-7 ensemble test-F1 = 0.992\n",
      "  rf  val-F1 = 0.379\n",
      "  cb  val-F1 = 0.396\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "  cnn val-F1 = 0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "  mlp val-F1 = 0.339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  svm val-F1 = 0.280\n",
      "  lr  val-F1 = 0.296\n",
      "  dt  val-F1 = 0.339\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "   ➞ Top-3 ensemble test-F1 = 0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
      "   ➞ Top-5 ensemble test-F1 = 0.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-7 ensemble test-F1 = 0.335\n",
      "  ⚠️  No classifiers loaded for “peak_memory”, skipping.\n",
      "→ saved ./binres_min_kp/ensembles_fl/greedy_min_class_ensembles.csv\n",
      "\n",
      "=== Solver: ga_min ===\n",
      "  rf  val-F1 = 0.704\n",
      "  cb  val-F1 = 0.951\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "  cnn val-F1 = 0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "  mlp val-F1 = 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  svm val-F1 = 0.925\n",
      "  lr  val-F1 = 0.648\n",
      "  dt  val-F1 = 0.953\n",
      "   ➞ Top-3 ensemble test-F1 = 0.981\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "   ➞ Top-5 ensemble test-F1 = 0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-7 ensemble test-F1 = 0.986\n",
      "  ⚠️  No classifiers loaded for “optimality_gap”, skipping.\n",
      "  rf  val-F1 = 1.000\n",
      "  cb  val-F1 = 1.000\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cnn val-F1 = 0.846\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "  mlp val-F1 = 0.823\n",
      "  svm val-F1 = 1.000\n",
      "  lr  val-F1 = 0.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dt  val-F1 = 1.000\n",
      "   ➞ Top-3 ensemble test-F1 = 1.000\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "   ➞ Top-5 ensemble test-F1 = 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n",
      "   ➞ Top-7 ensemble test-F1 = 1.000\n",
      "→ saved ./binres_min_kp/ensembles_fl/ga_min_class_ensembles.csv\n",
      "\n",
      "=== Solver: dp_min ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  rf  val-F1 = 0.711\n",
      "  cb  val-F1 = 0.811\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
      "  cnn val-F1 = 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "  mlp val-F1 = 0.731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  svm val-F1 = 0.693\n",
      "  lr  val-F1 = 0.676\n",
      "  dt  val-F1 = 0.642\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "   ➞ Top-3 ensemble test-F1 = 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-5 ensemble test-F1 = 0.651\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-7 ensemble test-F1 = 0.677\n",
      "  rf  val-F1 = 0.487\n",
      "  cb  val-F1 = 0.487\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "  cnn val-F1 = 0.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mlp val-F1 = 0.487\n",
      "  svm val-F1 = 0.487\n",
      "  lr  val-F1 = 0.487\n",
      "  dt  val-F1 = 0.737\n",
      "   ➞ Top-3 ensemble test-F1 = 1.000\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "   ➞ Top-5 ensemble test-F1 = 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "   ➞ Top-7 ensemble test-F1 = 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  rf  val-F1 = 0.556\n",
      "  cb  val-F1 = 0.556\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
      "  cnn val-F1 = 0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "  mlp val-F1 = 0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  svm val-F1 = 0.320\n",
      "  lr  val-F1 = 0.320\n",
      "  dt  val-F1 = 0.556\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-3 ensemble test-F1 = 0.556\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-5 ensemble test-F1 = 0.556\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step\n",
      "   ➞ Top-7 ensemble test-F1 = 0.556"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ saved ./binres_min_kp/ensembles_fl/dp_min_class_ensembles.csv\n",
      "\n",
      "=== Solver: bb_min ===\n",
      "  rf  val-F1 = 0.688\n",
      "  cb  val-F1 = 0.951\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "  cnn val-F1 = 0.853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "  mlp val-F1 = 0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  svm val-F1 = 0.925\n",
      "  lr  val-F1 = 0.693\n",
      "  dt  val-F1 = 0.951\n",
      "   ➞ Top-3 ensemble test-F1 = 0.967\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "   ➞ Top-5 ensemble test-F1 = 0.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-7 ensemble test-F1 = 0.967\n",
      "  rf  val-F1 = 0.524\n",
      "  cb  val-F1 = 0.486\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "  cnn val-F1 = 0.355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "  mlp val-F1 = 0.246\n",
      "  svm val-F1 = 0.313\n",
      "  lr  val-F1 = 0.232\n",
      "  dt  val-F1 = 0.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-3 ensemble test-F1 = 0.761\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➞ Top-5 ensemble test-F1 = 0.665\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step\n",
      "   ➞ Top-7 ensemble test-F1 = 0.590\n",
      "  ⚠️  No classifiers loaded for “peak_memory”, skipping.\n",
      "→ saved ./binres_min_kp/ensembles_fl/bb_min_class_ensembles.csv\n"
     ]
    }
   ],
   "source": [
    "run_all_models(BASE_DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb978b-eb1e-44a1-8b42-70fc7a5ede21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_classifiers_for_solver(solver, train_file, test_file, val_file):\n",
    "    print(f\"\\n=== Solver: {solver} ===\")\n",
    "   \n",
    "    df_tr = pd.read_csv(train_file).dropna()\n",
    "    df_va = pd.read_csv(val_file).dropna()\n",
    "    df_te = pd.read_csv(test_file).dropna()\n",
    "\n",
    "    # 2) load bin‐edges (to reconstruct y)\n",
    "    bins_dir = os.path.join(BINS_BASE, f\"{solver}_bins\")\n",
    "    binf = glob.glob(os.path.join(bins_dir, \"*_bins.json\"))\n",
    "    assert len(binf) == 1, \"Need exactly one JSON in \" + bins_dir\n",
    "    bin_edges = json.load(open(binf[0]))\n",
    "\n",
    "    # 3) scale features\n",
    "    feats = [\n",
    "        \"number_of_elements\", \"capacity\", \"max_weight\", \"min_weight\", \"mean_weight\",\n",
    "        \"median_weight\", \"std_weight\", \"weight_range\", \"max_profit\", \"min_profit\", \"mean_profit\",\n",
    "        \"median_profit\", \"std_profit\", \"profit_range\", \"renting_ratio\", \"mean_weight_profit_ratio\",\n",
    "        \"median_weight_profit_ratio\", \"capacity_mean_weight_ratio\", \"capacity_median_weight_ratio\",\n",
    "        \"capacity_std_weight_ratio\", \"std_weight_profit_ratio\", \"weight_profit_correlation\",\n",
    "        \"ram\", \"cpu_cores\"\n",
    "    ]\n",
    "    scaler = StandardScaler().fit(df_tr[feats])\n",
    "    X_va = scaler.transform(df_va[feats])\n",
    "    X_te = scaler.transform(df_te[feats])\n",
    "    X_va_cnn = X_va.reshape((-1, X_va.shape[1], 1))\n",
    "    X_te_cnn = X_te.reshape((-1, X_te.shape[1], 1))\n",
    "\n",
    "    records = []\n",
    "    for target in [\"solution_time\", \"optimality_gap\", \"peak_memory\"]:\n",
    "        # 4) recover true classes\n",
    "        edges = bin_edges[target]\n",
    "        to_bins = lambda arr: np.clip(np.digitize(arr, edges[:-1], right=False) - 1, 0, len(edges) - 2)\n",
    "        y_va_raw = to_bins(df_va[target].values)\n",
    "        y_te_raw = to_bins(df_te[target].values)\n",
    "\n",
    "        # 5) load each model’s val proba + compute F1\n",
    "        perf, proba = {}, {}\n",
    "        for name in [\"rf\", \"cb\", \"cnn\", \"mlp\", \"svm\", \"lr\", \"dt\"]:\n",
    "            nm, pr, path = load_classifier_proba(name, solver, target, X_va, X_va_cnn)\n",
    "            if pr is None:\n",
    "                continue\n",
    "\n",
    "            y_va = np.clip(y_va_raw.copy(), 0, pr.shape[1] - 1)\n",
    "            ypred = pr.argmax(axis=1)\n",
    "            perf[nm] = f1_score(y_va, ypred, average=\"macro\", zero_division=0)\n",
    "            proba[nm] = pr\n",
    "            print(f\"  {nm:<3} val-F1 = {perf[nm]:.3f}\")\n",
    "\n",
    "        if not perf:\n",
    "            print(f\"  ⚠️  No classifiers loaded for '{target}', skipping.\")\n",
    "            continue\n",
    "        # 6) rank by desc F1\n",
    "        ranked = sorted(perf, key=perf.get, reverse=True)\n",
    "\n",
    "        # 7) for each K, build ensemble on test\n",
    "        for K in K_LIST:\n",
    "            chosen = ranked[:K]\n",
    "            all_probas = [load_classifier_proba(m, solver, target, X_te, X_te_cnn)[1] for m in chosen]\n",
    "            n_cls = max(p.shape[1] for p in all_probas)\n",
    "\n",
    "            padded = []\n",
    "            for p in all_probas:\n",
    "                if p.shape[1] < n_cls:\n",
    "                    pad = np.zeros((p.shape[0], n_cls - p.shape[1]))\n",
    "                    p = np.hstack([p, pad])\n",
    "                padded.append(p)\n",
    "\n",
    "            P = np.stack(padded).mean(axis=0)\n",
    "            y_pred = P.argmax(axis=1)\n",
    "\n",
    "            acc = accuracy_score(np.clip(y_te_raw.copy(), 0, n_cls - 1), y_pred)\n",
    "            f1 = f1_score(np.clip(y_te_raw.copy(), 0, n_cls - 1), y_pred, average=\"macro\", zero_division=0)\n",
    "            print(f\"   ➞ Top-{K} ensemble test-F1 = {f1:.3f}\")\n",
    "\n",
    "            records.append({\n",
    "                \"solver\": solver,\n",
    "                \"target\": target,\n",
    "                \"Top_K\": K,\n",
    "                \"members\": \";\".join(chosen),\n",
    "                \"test_accuracy\": acc,\n",
    "                \"test_f1\": f1\n",
    "            })\n",
    "\n",
    "        # 8) SHAP bar-chart & heatmap for top-3 models\n",
    "        if len(ranked) >= 3:\n",
    "            top3 = ranked[:3]\n",
    "            shap_maps = []\n",
    "            for name in top3:\n",
    "                try:\n",
    "                    m = load_classifier_obj(name, solver, target)\n",
    "                    if name in (\"rf\", \"dt\", \"cb\"):\n",
    "                        expl = shap.TreeExplainer(m)\n",
    "                        sv = expl.shap_values(X_va)\n",
    "                        if isinstance(sv, list):\n",
    "                            arr = np.mean(np.abs(np.stack(sv, axis=0)), axis=(0, 1))  # (n_classes, n_samples, n_features) -> (n_features,)\n",
    "                        elif isinstance(sv, np.ndarray) and sv.ndim == 3:\n",
    "                            arr = np.mean(np.abs(sv), axis=(0, 1))  # (n_classes, n_samples, n_features)\n",
    "                        elif isinstance(sv, np.ndarray) and sv.ndim == 2:\n",
    "                            arr = np.mean(np.abs(sv), axis=0)  # (n_samples, n_features)\n",
    "                        else:\n",
    "                            raise ValueError(f\"Unexpected SHAP value shape: {sv.shape}\")\n",
    "\n",
    "                    else:\n",
    "                        if m in (\"mlp\", \"cnn\"):\n",
    "                            def keras_predict_proba(X):\n",
    "                                return m.predict(X, verbose=0)\n",
    "                            \n",
    "                            expl = shap.KernelExplainer(keras_predict_proba, bg)\n",
    "                            sv = expl.shap_values(X_va, nsamples=200)\n",
    "                        else:\n",
    "                        bg = shap.sample(X_va, 100)\n",
    "                        expl = shap.KernelExplainer(m.predict_proba, bg)\n",
    "                        sv = expl.shap_values(X_va, nsamples=200)\n",
    "                        arr = np.sum([np.mean(np.abs(s), axis=0) for s in sv], axis=0)\n",
    "                    shap_maps.append((name, arr))\n",
    "                except Exception as e:\n",
    "                    print(f\"  ⚠️  SHAP failed for {name}: {e}\")\n",
    "\n",
    "            good = [(n, a) for n, a in shap_maps if a.shape[0] == len(feats)]\n",
    "            if not good:\n",
    "                print(\"  ⚠️  no valid SHAP maps, skipping plots\")\n",
    "            else:\n",
    "                names, arrs = zip(*good)\n",
    "                M = np.stack(arrs, axis=1)\n",
    "                avg = M.mean(axis=1)\n",
    "\n",
    "                # pick top-10 features\n",
    "                idx_sorted = np.argsort(avg)\n",
    "                sel = idx_sorted[-10:][::-1]\n",
    "\n",
    "                # prepare labels and widths via numpy indexing\n",
    "                feats_arr = np.array(feats)\n",
    "                y_labels = feats_arr[sel].tolist()\n",
    "                widths = avg[sel].tolist()\n",
    "\n",
    "\n",
    "\n",
    "                # heatmap\n",
    "                plt.figure(figsize=(6,4))\n",
    "                plt.imshow(M, aspect=\"auto\")\n",
    "                plt.yticks(range(len(feats)), feats)\n",
    "                plt.xticks(range(len(names)), names, rotation=45)\n",
    "                plt.colorbar(label=\"Mean\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(FIG_DIR, f\"{solver}_{target}_heatmap.pdf\"))\n",
    "                plt.show()\n",
    "                plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb392f-f04b-451f-8da4-d11f908f22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models(base_folder):\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        for folder in dirs:\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            csv_files = os.listdir(folder_path)\n",
    "\n",
    "            train_file = [f for f in csv_files if f.endswith(\"_train.csv\")]\n",
    "            test_file = [f for f in csv_files if f.endswith(\"_test.csv\")]\n",
    "            val_file = [f for f in csv_files if f.endswith(\"_val.csv\")]\n",
    "\n",
    "            if train_file and test_file and val_file:\n",
    "                train_fp = os.path.join(folder_path, train_file[0])\n",
    "                test_fp = os.path.join(folder_path, test_file[0])\n",
    "                val_fp = os.path.join(folder_path, val_file[0])\n",
    "\n",
    "                solver_name = folder  \n",
    "                ensemble_classifiers_for_solver(solver_name, train_fp, test_fp, val_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a9bec-e659-413b-a1b8-5919f5d741de",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all_models(BASE_DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af0a11-8b89-4528-845b-290d0a7e723f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-gpu]",
   "language": "python",
   "name": "conda-env-torch-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
