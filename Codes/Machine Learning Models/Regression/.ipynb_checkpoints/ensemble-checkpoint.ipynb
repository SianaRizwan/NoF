{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487aaad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install catboost\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from itertools import combinations\n",
    "from tensorflow.keras.losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(y_true, y_pred, solver_name, target, model_combo, weights):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n[ENSEMBLE] Solver: {solver_name}, Target: {target}, Models: {model_combo}, Weights: {weights}\")\n",
    "    print(f\"MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "    return {\n",
    "        \"Solver\": solver_name,\n",
    "        \"Target\": target,\n",
    "        \"Models\": \"_\".join(model_combo),\n",
    "        \"Weights\": str(weights),\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7079d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_combination(model_name, solver_name, target, X_test, X_test_cnn):\n",
    "    try:\n",
    "        if model_name == \"rf\":\n",
    "            path = f\"./rf_reg/rf_models/rf_model_{solver_name}_{target}.joblib\"\n",
    "            if os.path.exists(path):\n",
    "                model = joblib.load(path)\n",
    "                return model_name, model.predict(X_test)\n",
    "        elif model_name == \"cb\":\n",
    "            path = f\"./cb_reg/cb_models/cb_model_{solver_name}_{target}.cbm\"\n",
    "            if os.path.exists(path):\n",
    "                model = CatBoostRegressor()\n",
    "                model.load_model(path)\n",
    "                return model_name, model.predict(X_test)\n",
    "        elif model_name == \"cnn\":\n",
    "            path = f\"./cnn_reg/cnn_models/cnn_model_{solver_name}_{target}.h5\"\n",
    "            if os.path.exists(path):\n",
    "                model = load_model(path, custom_objects={'mse': MeanSquaredError()})\n",
    "                return model_name, model.predict(X_test_cnn).flatten()\n",
    "        elif model_name == \"mlp\":\n",
    "            path = f\"./mlp_reg/mlp_models/mlp_model_{solver_name}_{target}.h5\"\n",
    "            if os.path.exists(path):\n",
    "                model = load_model(path, custom_objects={'mse': MeanSquaredError()})\n",
    "                return model_name, model.predict(X_test).flatten()\n",
    "        elif model_name == \"svm\":\n",
    "            path = f\"./svm_reg/svm_models/svm_model_{solver_name}_{target}.joblib\"\n",
    "            if os.path.exists(path):\n",
    "                model = joblib.load(path)\n",
    "                return model_name, model.predict(X_test)\n",
    "        elif model_name == \"lr\":\n",
    "            path = f\"./lr_reg/lr_models/lr_model_{solver_name}_{target}.joblib\"\n",
    "            if os.path.exists(path):\n",
    "                model = joblib.load(path)\n",
    "                return model_name, model.predict(X_test)\n",
    "        elif model_name == \"dt\":\n",
    "            path = f\"./dt_reg/dt_models/dt_model_{solver_name}_{target}.joblib\"\n",
    "            if os.path.exists(path):\n",
    "                model = joblib.load(path)\n",
    "                return model_name, model.predict(X_test)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {model_name} for {solver_name}, {target}: {e}\")\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c2d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def rank_models_by_performance(solver_name, test_file, target, X_test, X_test_cnn, y_test):\n",
    "    all_models = [\"rf\", \"cb\", \"cnn\", \"mlp\", \"svm\", \"lr\", \"dt\"]\n",
    "    model_performance = {}\n",
    "\n",
    "    for model_name in all_models:\n",
    "        name, pred = load_model_for_combination(model_name, solver_name, target, X_test, X_test_cnn)\n",
    "        if pred is not None:\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "            model_performance[model_name] = rmse\n",
    "            print(f\"{model_name} RMSE: {rmse:.4f}\")\n",
    "        else:\n",
    "            print(f\"{model_name} not evaluated.\")\n",
    "\n",
    "    # Sort models by RMSE\n",
    "    ranked_models = sorted(model_performance, key=model_performance.get)\n",
    "    print(f\"Models ranked by RMSE for {target}: {ranked_models}\")\n",
    "    return ranked_models, model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa839151-b8f6-435d-8f9d-591cf08a8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def run_top_k_ensemble_dynamic(solver_name, train_file, test_file):\n",
    "    \n",
    "    df_train = pd.read_csv(train_file)\n",
    "    df_test  = pd.read_csv(test_file)\n",
    "    df_test.dropna(inplace=True)\n",
    "\n",
    " \n",
    "    features = [\n",
    "        \"number_of_elements\",\"capacity\",\"max_weight\",\"min_weight\",\"mean_weight\",\n",
    "        \"median_weight\",\"std_weight\",\"weight_range\",\"max_profit\",\"min_profit\",\"mean_profit\",\n",
    "        \"median_profit\",\"std_profit\",\"profit_range\",\"renting_ratio\",\"mean_weight_profit_ratio\",\n",
    "        \"median_weight_profit_ratio\",\"capacity_mean_weight_ratio\",\"capacity_median_weight_ratio\",\n",
    "        \"capacity_std_weight_ratio\",\"std_weight_profit_ratio\",\"weight_profit_correlation\",\n",
    "        \"ram\",\"cpu_cores\"\n",
    "    ]\n",
    "    targets = [\"solution_time\",\"optimality_gap\",\"peak_memory\"]\n",
    "\n",
    "   \n",
    "    feat_scaler = StandardScaler().fit(df_train[features])\n",
    "    X_test      = feat_scaler.transform(df_test[features])\n",
    "    X_test_cnn  = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "  \n",
    "    y_scalers = {\n",
    "        t: StandardScaler().fit(df_train[[t]].values)\n",
    "        for t in targets\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for target in targets:\n",
    "        y_test = df_test[target].values\n",
    "\n",
    "       \n",
    "        ranked_models, _ = rank_models_by_performance(\n",
    "            solver_name, test_file, target, X_test, X_test_cnn, y_test\n",
    "        )\n",
    "\n",
    "        for k in [3, 5, 7]:\n",
    "            ensemble_preds = []\n",
    "            names = []\n",
    "\n",
    "            for mname in ranked_models[:k]:\n",
    "                name, pred = load_model_for_combination(\n",
    "                    mname, solver_name, target, X_test, X_test_cnn\n",
    "                )\n",
    "                if pred is None:\n",
    "                    continue\n",
    "\n",
    "               \n",
    "                if name in {\"cnn\",\"mlp\",\"dt\",\"rf\",\"svm\"}:\n",
    "                    pred = y_scalers[target].inverse_transform(pred.reshape(-1,1)).flatten()\n",
    "\n",
    "                ensemble_preds.append(pred)\n",
    "                names.append(name)\n",
    "\n",
    "            if not ensemble_preds:\n",
    "                print(f\"No models for Top-{k} on {solver_name}/{target}\")\n",
    "                continue\n",
    "\n",
    "            #Equal‚Äêweight average\n",
    "            P = np.vstack(ensemble_preds)\n",
    "            avg_pred = P.mean(axis=0)\n",
    "\n",
    "            row = evaluate_ensemble(\n",
    "                y_test, avg_pred, solver_name, target, tuple(names),\n",
    "                weights=[1/len(P)]*len(P)\n",
    "            )\n",
    "            row[\"Top_K\"] = k\n",
    "            results.append(row)\n",
    "\n",
    "    df_res = pd.DataFrame(results)\n",
    "    out_path = \"output.csv\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    df_res.to_csv(out_path, mode=\"a\", index=False,\n",
    "                  header=not os.path.exists(out_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58890091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensembles_for_all_solvers(base_folder):\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        for folder in dirs:\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            csv_files = os.listdir(folder_path)\n",
    "\n",
    "            train_file = [f for f in csv_files if f.endswith(\"_train.csv\")]\n",
    "            test_file = [f for f in csv_files if f.endswith(\"_test.csv\")]\n",
    "\n",
    "            if test_file:\n",
    "                train_fp = os.path.join(folder_path, train_file[0])\n",
    "                test_fp = os.path.join(folder_path, test_file[0])\n",
    "                solver_name = folder\n",
    "\n",
    "                print(f\"\\nRunning for: {solver_name}\")\n",
    "                run_top_k_ensemble_dynamic(solver_name, train_fp, test_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"./training_data\"  #Path to training data folder\n",
    "run_ensembles_for_all_solvers(base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c76c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
