{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aed307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_target(y, n_bins=5):\n",
    "    bins = np.linspace(min(y), max(y), n_bins + 1)\n",
    "    labels = list(range(n_bins))\n",
    "    y_binned = np.digitize(y, bins[:-1], right=False)\n",
    "    return y_binned, bins, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca5714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bin_accuracy(y_true, y_pred_bins, bins):\n",
    "    true_bins = np.digitize(y_true, bins[:-1], right=False)\n",
    "    correct = (true_bins == y_pred_bins)\n",
    "    return np.mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, results, solver_name, label, pred_bins, bins):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rel_rmse = rmse / np.mean(y_true)\n",
    "    \n",
    "    safe_y_true = np.where(y_true == 0, 1e-8, y_true)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / safe_y_true)) * 100 \n",
    "    bin_acc = compute_bin_accuracy(y_true, pred_bins, bins)\n",
    "\n",
    "    results.append({\n",
    "        \"Solver\": solver_name,\n",
    "        \"Dataset\": label,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"Rel_RMSE\": rel_rmse,\n",
    "        \"MAPE (%)\": mape,\n",
    "        \"BinAccuracy\": bin_acc\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22979d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(X_train, y_train, bins, alpha=0.1, gamma=0.9, epsilon=0.2, episodes=100):\n",
    "    n_actions = len(bins) - 1\n",
    "    q_table = defaultdict(lambda: np.zeros(n_actions))\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        total_reward = 0\n",
    "        for i in range(len(X_train)):\n",
    "            state = tuple(np.round(X_train[i], 2))\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(0, n_actions)\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])\n",
    "\n",
    "            pred_val = 0.5 * (bins[action] + bins[action + 1])\n",
    "            reward = -abs(pred_val - y_train[i])\n",
    "            total_reward += reward\n",
    "\n",
    "            q_table[state][action] += alpha * (\n",
    "                reward + gamma * np.max(q_table[state]) - q_table[state][action]\n",
    "            )\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "        if episode % 10 == 0 or episode == episodes - 1:\n",
    "            print(f\"Q-Learning Episode {episode}: Total Reward = {total_reward:.4f}\")\n",
    "\n",
    "            sample_state = tuple(np.round(X_train[0], 2))\n",
    "            print(f\"Q[{sample_state}] = {q_table[sample_state]}\")\n",
    "\n",
    "    # Plot rewards - Just for verification\n",
    "    # os.makedirs(\"./debug\", exist_ok=True)\n",
    "    # plt.figure(figsize=(7, 4))\n",
    "    # plt.plot(rewards_per_episode, label=\"Episode Reward\")\n",
    "    # plt.xlabel(\"Episode\")\n",
    "    # plt.ylabel(\"Total Reward\")\n",
    "    # plt.title(\"Q-Learning Reward Curve\")\n",
    "    # plt.grid(True)\n",
    "    # plt.legend()\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(\"./debug/q_learning_reward_curve.png\")\n",
    "    # plt.close()\n",
    "\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d525c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(X_train, y_train, bins, alpha=0.1, gamma=0.9, epsilon=0.2, episodes=100):\n",
    "    n_actions = len(bins) - 1\n",
    "    q_table = defaultdict(lambda: np.zeros(n_actions))\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        total_reward = 0\n",
    "        for i in range(len(X_train)):\n",
    "            state = tuple(np.round(X_train[i], 2))\n",
    "            action = np.random.randint(0, n_actions) if np.random.rand() < epsilon else np.argmax(q_table[state])\n",
    "\n",
    "            pred_val = 0.5 * (bins[action] + bins[action + 1])\n",
    "            reward = -abs(pred_val - y_train[i])\n",
    "            total_reward += reward\n",
    "\n",
    "            next_state = state  # single step\n",
    "            next_action = np.random.randint(0, n_actions) if np.random.rand() < epsilon else np.argmax(q_table[next_state])\n",
    "\n",
    "            q_table[state][action] += alpha * (\n",
    "                reward + gamma * q_table[next_state][next_action] - q_table[state][action]\n",
    "            )\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "        if episode % 10 == 0 or episode == episodes - 1:\n",
    "            print(f\"SARSA Episode {episode}: Total Reward = {total_reward:.4f}\")\n",
    "            sample_state = tuple(np.round(X_train[0], 2))\n",
    "            print(f\"Q[{sample_state}] = {q_table[sample_state]}\")\n",
    "\n",
    "     # Plot rewards - Just for verification\n",
    "    # os.makedirs(\"./debug\", exist_ok=True)\n",
    "    # plt.figure(figsize=(7, 4))\n",
    "    # plt.plot(rewards_per_episode, label=\"Episode Reward\")\n",
    "    # plt.xlabel(\"Episode\")\n",
    "    # plt.ylabel(\"Total Reward\")\n",
    "    # plt.title(\"SARSA Reward per Episode\")\n",
    "    # plt.grid(True)\n",
    "    # plt.legend()\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(\"./debug/sarsa_reward_curve.png\")\n",
    "    # plt.close()\n",
    "\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8fc1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_q_table(X, q_table, bins):\n",
    "    preds = []\n",
    "    pred_bins = []\n",
    "    for i in range(len(X)):\n",
    "        state = tuple(np.round(X[i], 2))\n",
    "        action = np.argmax(q_table[state])\n",
    "        pred = 0.5 * (bins[action] + bins[action + 1])\n",
    "        preds.append(pred)\n",
    "        pred_bins.append(action)\n",
    "    return preds, pred_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_predict(solver_name, train_file, test_file, val_file, strategy=\"q_learning\"):\n",
    "    print(f\"\\n[RL - {strategy.upper()} - Algo: {solver_name}]\")\n",
    "\n",
    "    df_train = pd.read_csv(train_file)\n",
    "    df_test = pd.read_csv(test_file)\n",
    "    df_val = pd.read_csv(val_file)\n",
    "    \n",
    "    df_train.dropna(inplace=True)\n",
    "    df_test.dropna(inplace=True)\n",
    "    df_val.dropna(inplace=True)\n",
    "\n",
    "    target_cols = [\"solution_time\", \"optimality_gap\", \"peak_memory\"]\n",
    "    features = [\n",
    "        \"number_of_elements\", \"capacity\", \"max_weight\", \"min_weight\", \"mean_weight\",\n",
    "        \"median_weight\", \"std_weight\", \"weight_range\", \"max_profit\", \"min_profit\", \"mean_profit\",\n",
    "        \"median_profit\", \"std_profit\", \"profit_range\", \"renting_ratio\", \"mean_weight_profit_ratio\",\n",
    "        \"median_weight_profit_ratio\", \"capacity_mean_weight_ratio\", \"capacity_median_weight_ratio\",\n",
    "        \"capacity_std_weight_ratio\", \"std_weight_profit_ratio\", \"weight_profit_correlation\",\n",
    "        \"ram\", \"cpu_cores\"\n",
    "    ]\n",
    "\n",
    "    for df in [df_train, df_test, df_val]:\n",
    "        for col in target_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset=[col], inplace=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(df_train[features])\n",
    "    X_test = scaler.transform(df_test[features])\n",
    "    X_val = scaler.transform(df_val[features])\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for target in target_cols:\n",
    "    \n",
    "        y_scaler = StandardScaler()\n",
    "        y_train_scaled = y_scaler.fit_transform(df_train[[target]]).flatten()\n",
    "        y_test_scaled = y_scaler.transform(df_test[[target]]).flatten()\n",
    "        y_val_scaled = y_scaler.transform(df_val[[target]]).flatten()\n",
    "\n",
    "        y_binned, bins, labels = discretize_target(y_train_scaled, n_bins=5)\n",
    "\n",
    "         # Tune hyperparameters \n",
    "        config = tune_rl_hyperparameters(X_train, y_train_scaled, X_val, y_val_scaled, bins, solver_name, target, strategy)\n",
    "        print(f\"Using tuned config for {target}: {config}\")\n",
    "\n",
    "        \n",
    "        if strategy == \"sarsa\":\n",
    "            q_table = train_sarsa(X_train, y_train_scaled, bins,\n",
    "                                  gamma=config[\"gamma\"],\n",
    "                                  epsilon=config[\"epsilon\"],\n",
    "                                  alpha=config[\"alpha\"],\n",
    "                                  episodes=config[\"episodes\"])\n",
    "        else:\n",
    "            q_table = train_q_learning(X_train, y_train_scaled, bins,\n",
    "                                       gamma=config[\"gamma\"],\n",
    "                                       epsilon=config[\"epsilon\"],\n",
    "                                       alpha=config[\"alpha\"],\n",
    "                                       episodes=config[\"episodes\"])\n",
    "\n",
    "\n",
    "        \n",
    "        pred_test_scaled, pred_bins_test = predict_with_q_table(X_test, q_table, bins)\n",
    "        pred_val_scaled, pred_bins_val = predict_with_q_table(X_val, q_table, bins)\n",
    "        pred_test = y_scaler.inverse_transform(np.array(pred_test_scaled).reshape(-1, 1)).flatten()\n",
    "        pred_val = y_scaler.inverse_transform(np.array(pred_val_scaled).reshape(-1, 1)).flatten()\n",
    "        y_test = y_scaler.inverse_transform(np.array(y_test_scaled).reshape(-1, 1)).flatten()\n",
    "        y_val = y_scaler.inverse_transform(np.array(y_val_scaled).reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"\\nTarget: {target.upper()}\")\n",
    "        print(\"[Test]\")\n",
    "        evaluate(y_test, pred_test, results, solver_name, f\"{target} (Test)\", pred_bins_test, bins)\n",
    "        print(\"[Val]\")\n",
    "        evaluate(y_val, pred_val, results, solver_name, f\"{target} (Val)\", pred_bins_val, bins)\n",
    "\n",
    "        print(f\"Accuracy (Test): {compute_bin_accuracy(y_test, pred_bins_test, bins):.4f}\")\n",
    "        print(f\"Accuracy (Val) : {compute_bin_accuracy(y_val, pred_bins_val, bins):.4f}\")\n",
    "\n",
    "        # Optionally save model\n",
    "        # joblib.dump(q_table, f\"./models/qtable_{strategy}_{solver_name}_{target}.pkl\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_file = f\"./results_min_kp/rl_{strategy}_eval_results.csv\"\n",
    "    if os.path.exists(results_file):\n",
    "        results_df.to_csv(results_file, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        results_df.to_csv(results_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_rl_hyperparameters(X_train, y_train, X_val, y_val, bins, solver_name, target, strategy):\n",
    "   \n",
    "\n",
    "    gammas = [0.9]\n",
    "    epsilons = [0.1, 0.01]\n",
    "    episodes_list = [500, 1000]\n",
    "    alphas = [0.05, 0.1]\n",
    "\n",
    "    param_grid = list(itertools.product(gammas, epsilons, episodes_list, alphas))\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_config = {}\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for gamma, epsilon, episodes, alpha in param_grid:\n",
    "        if strategy == \"sarsa\":\n",
    "            q_table = train_sarsa(X_train, y_train, bins, gamma=gamma, epsilon=epsilon,\n",
    "                                  alpha=alpha, episodes=episodes)\n",
    "        else:\n",
    "            q_table = train_q_learning(X_train, y_train, bins, gamma=gamma, epsilon=epsilon,\n",
    "                                       alpha=alpha, episodes=episodes)\n",
    "\n",
    "        pred_val_scaled, _ = predict_with_q_table(X_val, q_table, bins)\n",
    "        y_scaler = StandardScaler().fit(y_train.reshape(-1, 1))\n",
    "        pred_val = y_scaler.inverse_transform(np.array(pred_val_scaled).reshape(-1, 1)).flatten()\n",
    "        y_val_orig = y_scaler.inverse_transform(np.array(y_val).reshape(-1, 1)).flatten()\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_orig, pred_val))\n",
    "        all_results.append({\n",
    "            \"solver\": solver_name,\n",
    "            \"target\": target,\n",
    "            \"strategy\": strategy,\n",
    "            \"gamma\": gamma,\n",
    "            \"epsilon\": epsilon,\n",
    "            \"episodes\": episodes,\n",
    "            \"alpha\": alpha,\n",
    "            \"rmse\": rmse\n",
    "        })\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_config = {\n",
    "                \"gamma\": gamma,\n",
    "                \"epsilon\": epsilon,\n",
    "                \"episodes\": episodes,\n",
    "                \"alpha\": alpha\n",
    "            }\n",
    "\n",
    "    # Save tuning results\n",
    "    df_tuning = pd.DataFrame(all_results)\n",
    "    os.makedirs(\"./rl_tuning\", exist_ok=True)\n",
    "    df_tuning.to_csv(f\"./rl_tuning/tune_{strategy}_{solver_name}_{target}.csv\", index=False)\n",
    "\n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rl_models(base_folder, strategies=[\"q_learning\", \"sarsa\"]):\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        for folder in dirs:\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            csv_files = os.listdir(folder_path)\n",
    "\n",
    "            train_file = [f for f in csv_files if f.endswith(\"_train.csv\")]\n",
    "            test_file = [f for f in csv_files if f.endswith(\"_test.csv\")]\n",
    "            val_file = [f for f in csv_files if f.endswith(\"_val.csv\")]\n",
    "\n",
    "            if train_file and test_file and val_file:\n",
    "                train_fp = os.path.join(folder_path, train_file[0])\n",
    "                test_fp = os.path.join(folder_path, test_file[0])\n",
    "                val_fp = os.path.join(folder_path, val_file[0])\n",
    "\n",
    "                solver_name = folder\n",
    "                for strategy in strategies:\n",
    "                    rl_predict(solver_name, train_fp, test_fp, val_fp, strategy=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"./trainingData/final_td_min\" #Specify path to training data\n",
    "run_rl_models(base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d3ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
